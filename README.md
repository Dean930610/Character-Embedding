# Character-Embedding
This section mainly focuses on how to generate a character based word embedding based on Bi-LSTM based model. Most of the popular word embedding model generating methods mainly focus on the relationship between center words and surrouding words such as CBOW and Skip-gram methods. Word embedding vectors generated by these methods have been widely used in many NLP related practical works. Characeter embedding could represent the order of letters which might be important in some specific tasks. In many practical applicaiton, character based word embedding normally used with word embedding vectors together for seeking higher performance.  
In this section, a Bi-LSTM based neural network model can be used to acquire the character based word embedding. The target of this model is the word embedding vector of input words, and the inputs will be the sequencing letters of input word represented by one-hot encoding. After the neural network model reaching convergency, the hidden state of bidirectional LSTM layer can be extracted as the character based word embedding used for the requirements of classification or language modellign tasks.
### Main Procedure
#### 1. Data Pre-processing
The input of the Bi-LSTM model is the one-hot encoding of each character of input words and the target of this model is the word embedding vector of the corresponding word. In the preprocessing stage, it is essential to generate the word embedding model (self-trained or Glove and word2vec models), as well as to generate the one-hot encoding vector for generating input features of Bi-LSTM model.
#### 2. Build model Structure
A bidirectional LSTM network was employed to generate the character based word embedding model. Bidirectional stucture can effectively capture the context information around the center words or chars to make the generated word embedding vector become more meaningful. 
#### 3. Training 
Training the model to reach convergency. Try different hyperparameter values including learning rate, optimization methods and more. 
#### 4. Extracting the last hidden state
Extracting the hidden state of the traind model for each input features which can be used as  the character based word embedding model. 



