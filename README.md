# Character-Embedding
#### This section mainly focuses on how to generate a character based word embedding based on Bi-LSTM based model. The word embedding model will be used as the target of Bi-LSTM. The last hidden state of LSTM will be extracted as the character embedding vectors of input words.
#### Character embedding can be used in many NLP practical works as one type of input embedding for improving the performance of the algorithms. There are many data pre-processing approaches used which has been defined in the data pre-processing notenook which contains:
##### a. Change word to lower case and remove some punctuations. Although change all of the word to lower case could cause lossing some semantic meaning like 'China' and 'china', but in most of condition this is an effective way. The other process is aim to remove special characters and numerical token because they are useless for sentiment analysis.
##### b. Tokenization is the most important procedure to transform the test into tokens before transforming into word embedding vectors. It is the basic step for the further pre-processing procedures.
##### c. Removing stopping words is also an essential procedure for sentiment analysis. There are many words commonly occuring words which could not provide any useful senmantic meaning . NLTK provide a open-source library which could be used to remove the stop words for our dataset.
##### d. Leminization is the process to find the base or dictionary form of a word. This is similar to stemming but it could take into account the context of the word. In the previous section, the lemmazation function use the WordNetLemmatizer library to process the lemmazation process.
##### e. Padding is the process to ensure the input data of the neural network could have the same dimension.
##### f. The below chart shows most of the input corpus is below 130 words. This is why I select 130 as the padding size to avoid large input data size.
